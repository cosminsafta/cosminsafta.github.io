<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>News | Cosmin Safta</title>
    <link>https://cosminsafta.github.io/news/</link>
      <atom:link href="https://cosminsafta.github.io/news/index.xml" rel="self" type="application/rss+xml" />
    <description>News</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jun 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://cosminsafta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>News</title>
      <link>https://cosminsafta.github.io/news/</link>
    </image>
    
    <item>
      <title>Co-organizing minisymposium on &#34;Transfer Learning and Multi-Fidelity Approaches to Alleviate Data Sparsity in Machine Learning&#34; at SIAM-MDS22</title>
      <link>https://cosminsafta.github.io/news/2022-06-01/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://cosminsafta.github.io/news/2022-06-01/</guid>
      <description>&lt;p&gt;Mini-symposium Abstract: Machine learning (ML) models of sufficient predictive accuracy often rely on a significant volume of data for the inverse problem of model calibration. This may be limiting due to, for example, prohibitive expense of computer simulations or lack of field/experimental data. ML models have been mainly applied to tasks and domains that, while impactful, have sufficient volume of data. However, when deploying ML models for scientific or engineering tasks, they are sometimes invoked in conditions that do not overlap the set of scenarios for which the model was trained, or in scenarios with insufficient high-fidelity data for training purposes. State-of-the-art ML models, despite exhibiting superior performance on the domain they were trained on, suffer detrimental loss in performance in such extrapolatory or data-sparse settings. This loss in performance is also unpredictable and sensitive to both the amount and nature of data sparsity.&lt;/p&gt;
&lt;p&gt;Transfer learning is the process in which knowledge gained through similar training tasks is used to improve the training process on a new task, possibly suffering from limited data. Alternatively, data of lower levels of fidelity might improve the training task of interest by supplementing sparse high-fidelity data through multi-fidelity training frameworks. This mini-symposium will focus on novel methodologies and applications of transfer learning and multi-fidelity data fusion to enhance the performance of ML models in sparse data settings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Co-organizing minisymposium on &#34;Incorporating fundamental principles in innovative machine learning models of physics&#34; at WCCM22</title>
      <link>https://cosminsafta.github.io/news/2021-03-02/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://cosminsafta.github.io/news/2021-03-02/</guid>
      <description>&lt;p&gt;Theoretical and algorithmic advances in neural networks and related representations have begun to revolutionize the field of computational physics. To realize the potential of these algorithms, the field of scientific machine learning (SciML) has begun to combine the mature numerical analysis and algorithms from computational physics with innovative machine learning coming from other disciplines. As with the development of more traditional schemes such as energy preserving time integrators and mass conserving discretizations, physics informed machine learning is now expected to preserve fundamental features of the physical problem such as conservation, well-posedness, stability, symmetries, and invariants in order to provide robust and trustworthy predictions. We solicit talks that attack some of the fundamental open questions in SciML, such as how to embed physical constraints, how to employ the best aspects of traditional numerical methods and SciML, and how to ensure regular convergence and robustness.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Co-organizing minisymposium on &#34;Uncertainty Quantification and Propagation in Climate Models&#34; at SIAM-UQ22</title>
      <link>https://cosminsafta.github.io/news/2022-03-01/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://cosminsafta.github.io/news/2022-03-01/</guid>
      <description>&lt;p&gt;Uncertainty quantification (UQ) and propagation methods are facing a number of challenges when dealing with climate models. The curse of dimensionality, manifested by both the large number of input parameters and dense spatio-temporal outputs, together with the computational expense and non-linear behavior of components of climate models necessitates UQ method development on top of off-the-shelf approaches. This minisymposium highlights recent work on forward UQ method development, including techniques for uncertainty propagation and global sensitivity analysis, targeting the challenges above in the context of deployment for various components of earth system modeling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Submitted white paper to AI4ESP</title>
      <link>https://cosminsafta.github.io/news/2021-04/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://cosminsafta.github.io/news/2021-04/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sandia National Labs&#39; 2020 HPC Annual Report Released</title>
      <link>https://cosminsafta.github.io/news/2021-03-01/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://cosminsafta.github.io/news/2021-03-01/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
