[{"authors":null,"categories":null,"content":"I am a Distinguished Member of Technical Staff at Sandia National Laboratories. My research focuses on computational science with 20+ years of experience in developing algorithms for a large set of applications. I am involved in and leading research projects pertaining to uncertainty quantification and machine learning in combustion, climate modeling, biosurveillance, power grids, material science, stochastic network models, and re-entry vehicle trajectories, as well as statistical modeling related to adversarial aspects in machine learning.\n  Download my resumé.\n","date":1654041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1654041600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Distinguished Member of Technical Staff at Sandia National Laboratories. My research focuses on computational science with 20+ years of experience in developing algorithms for a large set of applications.","tags":null,"title":"Cosmin Safta","type":"authors"},{"authors":[],"categories":null,"content":"","date":1659225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659225600,"objectID":"1349a54140b52d2e0efb1b71a454a805","permalink":"https://cosminsafta.github.io/talk/machine-learning-constitutive-models-of-inelastic-materials-with-microstructure/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/talk/machine-learning-constitutive-models-of-inelastic-materials-with-microstructure/","section":"event","summary":"Graph-based neural networks designed to model the response of elastic-plastic and viscoelastic materials with pores.","tags":[],"title":"Machine learning constitutive models of inelastic materials with microstructure","type":"event"},{"authors":[],"categories":null,"content":"","date":1656201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656201600,"objectID":"9a6b899ecf39f321e4645ddea1599a69","permalink":"https://cosminsafta.github.io/talk/development-of-a-spatially-regularized-detector-for-emergent/re-emergent-disease-outbreaks/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/talk/development-of-a-spatially-regularized-detector-for-emergent/re-emergent-disease-outbreaks/","section":"event","summary":"Bayesian framework to detect outbreaks of emerging diseases.","tags":[],"title":"Development of a spatially regularized detector for emergent/re-emergent disease outbreaks","type":"event"},{"authors":["Cosmin Safta"],"categories":["minisimposium"],"content":"Mini-symposium Abstract: Machine learning (ML) models of sufficient predictive accuracy often rely on a significant volume of data for the inverse problem of model calibration. This may be limiting due to, for example, prohibitive expense of computer simulations or lack of field/experimental data. ML models have been mainly applied to tasks and domains that, while impactful, have sufficient volume of data. However, when deploying ML models for scientific or engineering tasks, they are sometimes invoked in conditions that do not overlap the set of scenarios for which the model was trained, or in scenarios with insufficient high-fidelity data for training purposes. State-of-the-art ML models, despite exhibiting superior performance on the domain they were trained on, suffer detrimental loss in performance in such extrapolatory or data-sparse settings. This loss in performance is also unpredictable and sensitive to both the amount and nature of data sparsity.\nTransfer learning is the process in which knowledge gained through similar training tasks is used to improve the training process on a new task, possibly suffering from limited data. Alternatively, data of lower levels of fidelity might improve the training task of interest by supplementing sparse high-fidelity data through multi-fidelity training frameworks. This mini-symposium will focus on novel methodologies and applications of transfer learning and multi-fidelity data fusion to enhance the performance of ML models in sparse data settings.\n","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"a0e79167926964f7bf753041872f4dcb","permalink":"https://cosminsafta.github.io/news/2022-06-01/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/news/2022-06-01/","section":"news","summary":"https://meetings.siam.org/program.cfm?CONFCODE=mds22","tags":["Academic"],"title":"Co-organizing minisymposium on \"Transfer Learning and Multi-Fidelity Approaches to Alleviate Data Sparsity in Machine Learning\" at SIAM-MDS22","type":"news"},{"authors":[],"categories":null,"content":"I am contributing to the following presentations at this conference:\n Arun S. Hegde, Elan Weiss, Wolfgang Windl, Habib N. Najm, Cosmin Safta, Bayesian Calibration of Interatomic Potential Models for Binary Alloys Luke Boll, Katherine M. Johnston, Khachik Sargsyan, Cosmin Safta, Bert J. Debusschere, The UQTk C++/Python Toolkit for Uncertainty Quantification: Overview and Applications Ari Frankel, Cosmin Safta, Reese Jones, Graph Convolutional Neural Networks for Microstructure Homogenization with Quantified Uncertainty Daniel Ricciuto, Khachik Sargsyan, Cosmin Safta, Spatio-Temporal Land Model Calibration Using Karhunen-Loeve Expansions  ","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"a3d2ba076093b036e102aff842ba5bde","permalink":"https://cosminsafta.github.io/talk/quantifying-uncertainty-in-e3sm-via-functional-tensor-network-approximations/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/talk/quantifying-uncertainty-in-e3sm-via-functional-tensor-network-approximations/","section":"event","summary":"Employ low-rank functional tensor network models for UQ in earth system models.","tags":[],"title":"Quantifying Uncertainty in E3SM via Functional Tensor Network Approximations","type":"event"},{"authors":null,"categories":null,"content":"We develop graph-based convolutional neural networks to provide efficient homogenized surrogate and full-field models for a wide class of physical problems that involve complex initial states. Graph neural networks have advantages over the more established neural network architectures that have been applied to physical problems. Specifically, we will embed spatially invariant and conserving feature representations and filters into graph architectures. This aspect will reduce the raw high-fidelity mesh inputs to more compact, low-rank manifolds since the symmetries will constrain the manifolds of features related to the chosen outputs. More broadly, our machine learning approach will be guided by expert knowledge from a multitude of more mature fields of mathematical and physical science, including signal processing and representation theory. This guided but still general learning framework should be more generalizable and more interpretable than standard approaches.\n","date":1647734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647734400,"objectID":"2d8a138badb1d8468fb238c907a97d58","permalink":"https://cosminsafta.github.io/project/gcnn/","publishdate":"2022-03-20T00:00:00Z","relpermalink":"/project/gcnn/","section":"project","summary":"Graph Neural Network Models of Complex Initial Boundary Value Problems that embed Physical Invariances","tags":null,"title":"Graph Neural Network Models","type":"project"},{"authors":null,"categories":null,"content":"The FASTMath SciDAC Institute develops and deploys scalable mathematical algorithms and software tools for reliable simulation of complex physical phenomena and collaborates with application scientists to ensure the usefulness and applicability of FASTMath technologies. Our work focuses on the development of functional tensor network surrogates to address challenges related to sparse training data, nonlinear models, and high-dimensionality.\n","date":1647734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647734400,"objectID":"53da262f7db479738491da76026e86b1","permalink":"https://cosminsafta.github.io/project/fastmath/","publishdate":"2022-03-20T00:00:00Z","relpermalink":"/project/fastmath/","section":"project","summary":"Functional tensor network models.","tags":null,"title":"Tensor and Bayesian Framework for Machine Learning","type":"project"},{"authors":["Cosmin Safta"],"categories":["minisimposium"],"content":"Theoretical and algorithmic advances in neural networks and related representations have begun to revolutionize the field of computational physics. To realize the potential of these algorithms, the field of scientific machine learning (SciML) has begun to combine the mature numerical analysis and algorithms from computational physics with innovative machine learning coming from other disciplines. As with the development of more traditional schemes such as energy preserving time integrators and mass conserving discretizations, physics informed machine learning is now expected to preserve fundamental features of the physical problem such as conservation, well-posedness, stability, symmetries, and invariants in order to provide robust and trustworthy predictions. We solicit talks that attack some of the fundamental open questions in SciML, such as how to embed physical constraints, how to employ the best aspects of traditional numerical methods and SciML, and how to ensure regular convergence and robustness.\n","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647216000,"objectID":"43474bfc9147c4ae3cd962343e03f6d9","permalink":"https://cosminsafta.github.io/news/2021-03-02/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/news/2021-03-02/","section":"news","summary":"https://www.wccm2022.org","tags":["Academic"],"title":"Co-organizing minisymposium on \"Incorporating fundamental principles in innovative machine learning models of physics\" at WCCM22","type":"news"},{"authors":["Cosmin Safta"],"categories":["minisimposium"],"content":"Uncertainty quantification (UQ) and propagation methods are facing a number of challenges when dealing with climate models. The curse of dimensionality, manifested by both the large number of input parameters and dense spatio-temporal outputs, together with the computational expense and non-linear behavior of components of climate models necessitates UQ method development on top of off-the-shelf approaches. This minisymposium highlights recent work on forward UQ method development, including techniques for uncertainty propagation and global sensitivity analysis, targeting the challenges above in the context of deployment for various components of earth system modeling.\n","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"cb94d9d3a39216ec0c9c3a5d02102875","permalink":"https://cosminsafta.github.io/news/2022-03-01/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/news/2022-03-01/","section":"news","summary":"https://www.siam.org/conferences/cm/conference/uq22","tags":["Academic"],"title":"Co-organizing minisymposium on \"Uncertainty Quantification and Propagation in Climate Models\" at SIAM-UQ22","type":"news"},{"authors":[],"categories":null,"content":"I am contributing to the following presentations at this conference:\n AKhachik Sargsyan, Daniel M Ricciuto, Cosmin Safta, Hit twice by the curse of dimensionality: spatio-temporal land model calibration using Karhunen-Loève and sparse polynomial chaos expansions  ","date":1639353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639353600,"objectID":"37fa5810d0a890d1d407839c726d6f00","permalink":"https://cosminsafta.github.io/talk/functional-tensor-network-approximations-for-e3sm-land-model/","publishdate":"2021-12-13T00:00:00Z","relpermalink":"/talk/functional-tensor-network-approximations-for-e3sm-land-model/","section":"event","summary":"Employ low-rank functional tensor network models for global sensitivity analysis at a regional scale.","tags":[],"title":"Functional Tensor Network Approximations for E3SM Land Model","type":"event"},{"authors":["P. Blonigan","J. Ray","C. Safta"],"categories":null,"content":"","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635206400,"objectID":"b450bfaeb41ad3e211f59fc261d6a4ed","permalink":"https://cosminsafta.github.io/publication/blonigan-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/publication/blonigan-2021/","section":"publication","summary":"We present a simple, near-real-time Bayesian method to infer and forecast a multiwave outbreak, and demonstrate it on the COVID-19 pandemic. The approach uses timely epidemiological data that has been widely available for COVID-19. It provides short-term forecasts of the outbreak’s evolution, which can then be used for medical resource planning. The method postulates one- and multiwave infection models, which are convolved with the incubation-period distribution to yield competing disease models. The disease models’ parameters are estimated via Markov chain Monte Carlo sampling and information-theoretic criteria are used to select between them for use in forecasting. The method is demonstrated on two- and three-wave COVID-19 outbreaks in California, New Mexico and Florida, as observed during Summer-Winter 2020. We find that the method is robust to noise, provides useful forecasts (along with uncertainty bounds) and that it reliably detected when the initial single-wave COVID-19 outbreaks transformed into successive surges as containment efforts in these states failed by the end of Spring 2020.","tags":null,"title":"Forecasting Multi-Wave Epidemics Through Bayesian Inference","type":"publication"},{"authors":["O. Diaz-Ibarra","K. Kim","C. Safta","J. Zador","H.N. Najm"],"categories":null,"content":"","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635206400,"objectID":"96f362c9b939b412191fd1ec9d5f1ccf","permalink":"https://cosminsafta.github.io/publication/diaz-ibarra-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/publication/diaz-ibarra-2021/","section":"publication","summary":"We have extended the computational singular perturbation (CSP) method to differential algebraic equation (DAE) systems and demonstrated its application in a heterogeneous-catalysis problem. The extended method obtains the CSP basis vectors for DAEs from a reduced Jacobian matrix that takes the algebraic constraints into account. We use a canonical problem in heterogeneous catalysis, the transient continuous stirred tank reactor (T-CSTR), for illustration. The T-CSTR problem is modelled fundamentally as an ordinary differential equation (ODE) system, but it can be transformed to a DAE system if one approximates typically fast surface processes using algebraic constraints for the surface species. We demonstrate the application of CSP analysis for both ODE and DAE constructions of a T-CSTR problem, illustrating the dynamical response of the system in each case. We also highlight the utility of the analysis in commenting on the quality of any particular DAE approximation built using the quasi-steady state approximation (QSSA), relative to the ODE reference case.","tags":null,"title":"Using computational singular perturbation as a diagnostic tool in ODE and DAE systems: a case study in heterogeneous catalysis","type":"publication"},{"authors":[],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"4517c0e256bf6a457593cd1b432ae869","permalink":"https://cosminsafta.github.io/talk/machine-learning-constitutive-models/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/talk/machine-learning-constitutive-models/","section":"event","summary":"Machine-learned models that capture the microstructure impact on mechanical outcomes","tags":[],"title":"Machine learning constitutive models","type":"event"},{"authors":["A.L. Frankel","C. Safta","C. Alleman","R. Jones"],"categories":null,"content":"","date":1632182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632182400,"objectID":"7410440e66306a77c2cb413b03c8dc2d","permalink":"https://cosminsafta.github.io/publication/frankel-2022/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/frankel-2022/","section":"publication","summary":"Predicting the evolution of a representative sample of a material with microstructure is a fundamental problem in homogenization. In this work we propose a graph convolutional neural network that utilizes the discretized representation of the initial microstructure directly, without segmentation or clustering. Compared to feature-based and pixel-based convolutional neural network models, the proposed method has a number of advantages: (a) it is deep in that it does not require featurization but can benefit from it, (b) it has a simple implementation with standard convolutional filters and layers, (c) it works natively on unstructured and structured grid data without interpolation (unlike pixel-based convolutional neural networks), and (d) it preserves rotational invariance like other graph-based convolutional neural networks. We demonstrate the performance of the proposed network and compare it to traditional pixel-based convolution neural network models and feature-based graph convolutional neural networks on multiple large datasets.","tags":null,"title":"Mesh-based Graph Convolutional Neural Networks For Modeling Materials with Microstructure","type":"publication"},{"authors":[],"categories":null,"content":"I am contributing to the following presentations at this conference:\n Kelli McCoy, Cosmin Safta, Roger Ghanem, Manifold-Based Optimization for Constrained Trajectoriess Aniket Jivani, Xun Huan, Cosmin Safta, Beckett Y. Zhou, Nicolas R. Gauger, Uncertainty Quantification for Random Field Quantities Using Multifidelity Karhunen-Loeve Expansions Arun Hegde, Elan Weiss, Wolfgang Windl, Habib Najm, Cosmin Safta, Bayesian Calibration of Interatomic Potential Models for Binary Alloys  ","date":1627516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627516800,"objectID":"4012d5393584cf16534ec2d073ba94c1","permalink":"https://cosminsafta.github.io/talk/functional-tensor-network-approximations-for-earth-system-models/","publishdate":"2021-07-29T00:00:00Z","relpermalink":"/talk/functional-tensor-network-approximations-for-earth-system-models/","section":"event","summary":"Employ low-rank functional tensor network models for UQ in earth system models.","tags":[],"title":"Functional Tensor Network Approximations for Earth System Models","type":"event"},{"authors":["K. Lee","J. Ray","C. Safta"],"categories":null,"content":"","date":1624492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624492800,"objectID":"5faa13f5e9ecb225628889c937f9a0ca","permalink":"https://cosminsafta.github.io/publication/lee-2021/","publishdate":"2021-07-09T00:00:00Z","relpermalink":"/publication/lee-2021/","section":"publication","summary":"In this paper we investigate the utility of one-dimensional convolutional neural network (CNN) models in epidemiological forecasting. Deep learning models, in particular variants of recurrent neural networks (RNNs) have been studied for ILI (Influenza-Like Illness) forecasting, and have achieved a higher forecasting skill compared to conventional models such as ARIMA. In this study, we adapt two neural networks that employ one-dimensional temporal convolutional layers as a primary building block—temporal convolutional networks and simple neural attentive meta-learners—for epidemiological forecasting. We then test them with influenza data from the US collected over 2010-2019. We find that epidemiological forecasting with CNNs is feasible, and their forecasting skill is comparable to, and at times, superior to, plain RNNs. Thus CNNs and RNNs bring the power of nonlinear transformations to purely data-driven epidemiological models, a capability that heretofore has been limited to more elaborate mechanistic/compartmental disease models.","tags":null,"title":"The predictive skill of convolutional neural networks models for disease forecasting","type":"publication"},{"authors":["Cosmin Safta"],"categories":["white paper"],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"83d9395f4c84f27bb638085fad67239b","permalink":"https://cosminsafta.github.io/news/2021-04/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/news/2021-04/","section":"news","summary":"White papers were solicited for development and application of AI methods in areas relevant to EESSD research with an emphasis on quantifying and improving Earth system predictability, particularly related to the integrative water cycle and extreme events. Our [white paper](https://www.ai4esp.org/files/AI4ESP1117_Sargsyan_Khachik.pdf) focuses on improving the earth system model through uncertainty attribution and active learning.","tags":["Academic"],"title":"Submitted white paper to AI4ESP","type":"news"},{"authors":["Y.T. Lin","J. Neumann","E.F. Miller","R.G. Posner","A. Mallela","C. Safta","J. Ray","G. Thakur","S. Chinthavali","W.S. Hlavacek"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"ddf6ad4a153ef807cdf6c8145608a7af","permalink":"https://cosminsafta.github.io/publication/lin-2021/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/lin-2021/","section":"publication","summary":"To increase situational awareness and support evidence-based policymaking, we formulated a mathematical model for coronavirus disease transmission within a regional population. This compartmental model accounts for quarantine, self-isolation, social distancing, a nonexponentially distributed incubation period, asymptomatic persons, and mild and severe forms of symptomatic disease. We used Bayesian inference to calibrate region-specific models for consistency with daily reports of confirmed cases in the 15 most populous metropolitan statistical areas in the United States. We also quantified uncertainty in parameter estimates and forecasts. This online learning approach enables early identification of new trends despite considerable variability in case reporting.","tags":null,"title":"Daily Forecasting of Regional Epidemics of Coronavirus Disease with Bayesian Uncertainty Quantification, United States","type":"publication"},{"authors":["Cosmin Safta"],"categories":["white paper"],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"f925b9f9267a699a26e1fe26e604c8b4","permalink":"https://cosminsafta.github.io/news/2021-03-01/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/news/2021-03-01/","section":"news","summary":"Here is a short video describing our project ([link](https://youtu.be/lMiNSziud6Y)).","tags":["Academic"],"title":"Sandia National Labs' 2020 HPC Annual Report Released","type":"news"},{"authors":["L.P. Swiler","M. Gulian","A.L. Frankel","C. Safta","J.D. Jakeman"],"categories":null,"content":"","date":1600819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600819200,"objectID":"2c04ad30418168b79c9963ab9bb6d91b","permalink":"https://cosminsafta.github.io/publication/swiler-2020/","publishdate":"2020-09-23T00:00:00Z","relpermalink":"/publication/swiler-2020/","section":"publication","summary":"Gaussian process regression is a popular Bayesian framework for surrogate modeling of expensive data sources. As part of a broader effort in scientific machine learning, many recent works have incorporated physical constraints or other a priori information within Gaussian process regression to supplement limited data and regularize the behavior of the model. We provide an overview and survey of several classes of Gaussian process constraints, including positivity or bound constraints, monotonicity and convexity constraints, differential equation constraints provided by linear PDEs, and boundary condition constraints. We compare the strategies behind each approach as well as the differences in implementation, concluding with a discussion of the computational challenges introduced by constraints.","tags":null,"title":"A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges","type":"publication"},{"authors":["C. Safta","J. Ray","K. Sargsyan"],"categories":null,"content":"","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"d6d90050584994b34d397527cc313f4d","permalink":"https://cosminsafta.github.io/publication/safta-2020/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/safta-2020/","section":"publication","summary":"We demonstrate a Bayesian method for the “real-time” characterization and forecasting of partially observed COVID-19 epidemic. Characterization is the estimation of infection spread parameters using daily counts of symptomatic patients. The method is designed to help guide medical resource allocation in the early epoch of the outbreak. The estimation problem is posed as one of Bayesian inference and solved using a Markov chain Monte Carlo technique. The data used in this study was sourced before the arrival of the second wave of infection in July 2020. The proposed modeling approach, when applied at the country level, generally provides accurate forecasts at the regional, state and country level. The epidemiological model detected the flattening of the curve in California, after public health measures were instituted. The method also detected different disease dynamics when applied to specific regions of New Mexico.","tags":null,"title":"Characterization of partially observed epidemics through Bayesian inference: Application to COVID-19","type":"publication"},{"authors":["H. Lu","Q. Shen","J. Che","X Wu","X. Fu","M. Khalil","C. Safta","Y. Huang"],"categories":null,"content":"","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"5ac9adb1b5702d31ab03190ffb6b0dbe","permalink":"https://cosminsafta.github.io/publication/lu-2020/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/publication/lu-2020/","section":"publication","summary":"Solving a nonlinear inverse problem is challenging in computational science and engineering. Sampling-based methods require a large number of model evaluations; gradient-based methods require fewer model evaluations but only find the local minima. Multifidelity optimization combines the low-fidelity model and the high-fidelity model to achieve both high accuracy and high efficiency. In this article, we present a bifidelity approach to solve nonlinear inverse problems. In the bifidelity inversion method, the low-fidelity model is used to acquire a good initial guess, and the high-fidelity model is used to locate the global minimum. Combined with a multistart optimization scheme, the proposed approach significantly increases the possibility of finding the global minimum for nonlinear inverse problems with many local minima. The method is tested with two toy problems and then applied to an electromagnetic well-logging inverse problem, which is difficult to solve using traditional gradient-based methods. The bifidelity method provides promising inversion results and can be easily applied to traditional gradient-based methods.","tags":null,"title":"Bifidelity Gradient-Based Approach for Nonlinear Well-Logging Inverse Problems","type":"publication"},{"authors":["Z.J. Buras","C. Safta","J. Zador","L. Sheps"],"categories":null,"content":"","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"ca5557d38a7aa8c37d512bdd7aaee7c9","permalink":"https://cosminsafta.github.io/publication/buras-2020/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/publication/buras-2020/","section":"publication","summary":"Chemical kinetics simulations are used to explore whether detailed measurements of relevant chemical species during the oxidation of very dilute fuels (less than 1 Torr partial pressure) in a high-pressure plug flow reactor (PFR) can predict autoignition propensity. We find that for many fuels the timescale for the onset of spontaneous oxidation in dilute fuel/air mixtures in a simple PFR is similar to the 1st-stage ignition delay time (IDT) at stoichiometric engine-relevant conditions. For those fuels that deviate from this simple trend, the deviation is closely related to the peak rate of production of OH, HO2, CH2O, and CO2 formed during oxidation. We use these insights to show that an accurate correlation between simulated profiles of these species in a PFR and 1st-stage IDT can be developed using convolutional neural networks. Our simulations suggest that the accuracy of such a correlation is 10–50%, which is appropriate for rapid fuel screening and may be sufficient for predictive fuel performance modeling.","tags":null,"title":"Simulated production of OH, HO2, CH2O, and CO2 during dilute fuel oxidation can predict 1st-stage ignition delays","type":"publication"},{"authors":["R.G. Ghanem","C. Soize","C. Safta","X. Huan","G. Lacaze","J.C. Oefelein","H.N. Najm"],"categories":null,"content":"","date":1566172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566172800,"objectID":"b7f82f2282a01bd6889fe573b7e48a0a","permalink":"https://cosminsafta.github.io/publication/ghanem-2019a/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/publication/ghanem-2019a/","section":"publication","summary":"We demonstrate, on a scramjet combustion problem, a constrained probabilistic learning approach that augments physics-based datasets with realizations that adhere to underlying constraints and scatter. The constraints are captured and delineated through diffusion maps, while the scatter is captured and sampled through a projected stochastic differential equation. The objective function and constraints of the optimization problem are then efficiently framed as non-parametric conditional expectations. Different spatial resolutions of a large-eddy simulation filter are used to explore the robustness of the model to the training dataset and to gain insight into the significance of spatial resolution on optimal design.","tags":null,"title":"Design optimization of a scramjet under uncertainty using probabilistic learning on manifolds","type":"publication"},{"authors":["C. Soize","R. Ghanem","C. Safta","X. Huan","Z.P. Vane","J. Oefelein","G. Lacaze","H.N. Najm","Q. Tang","X. Chen"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"dbe021186a5d89084376637147ec423a","permalink":"https://cosminsafta.github.io/publication/soize-2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/soize-2019/","section":"publication","summary":"In a recent paper, the authors proposed a general methodology for probabilistic learning on manifolds. The method was used to generate numerical samples that are statistically consistent with an existing dataset construed as a realization from a non-Gaussian random vector. The manifold structure is learned using diffusion manifolds and the statistical sample generation is accomplished using a projected Itô stochastic differential equation. This probabilistic learning approach has been extended to polynomial chaos representation of databases on manifolds and to probabilistic nonconvex constrained optimization with a fixed budget of function evaluations. The methodology introduces an isotropic-diffusion kernel with hyperparameter ε. Currently, ε is more or less arbitrarily chosen. In this paper, we propose a selection criterion for identifying an optimal value of ε, based on a maximum entropy argument. The result is a comprehensive, closed, probabilistic model for characterizing data sets with hidden constraints. This entropy argument ensures that out of all possible models, this is the one that is the most uncertain beyond any specified constraints, which is selected. Applications are presented for several databases.","tags":null,"title":"Entropy-based closure for probabilistic learning on manifolds","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://cosminsafta.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["C. Soize","R. Ghanem","C. Safta","X. Huan","Z.P. Vane","J.C. Oefelein","G Lacaze","H.N. Najm"],"categories":null,"content":"","date":1544745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544745600,"objectID":"1e0a88865927c99023613d6d6bb90826","permalink":"https://cosminsafta.github.io/publication/soize-2019b/","publishdate":"2018-12-14T00:00:00Z","relpermalink":"/publication/soize-2019b/","section":"publication","summary":"The computational burden of a large-eddy simulation for reactive flows is exacerbated in the presence of uncertainty in flow conditions or kinetic variables. A comprehensive statistical analysis, with a sufficiently large number of samples, remains elusive. Statistical learning is an approach that allows for extracting more information using fewer samples. Such procedures, if successful, will greatly enhance the predictability of models in the sense of improving exploration and characterization of uncertainty due to model error and input dependencies, all while being constrained by the size of the associated statistical samples. In this paper, it is shown how a recently developed procedure for probabilistic learning on manifolds can serve to improve the predictability in a probabilistic framework of a scramjet simulation. The estimates of the probability density functions of the quantities of interest are improved together with estimates of the statistics of their maxima. It is also demonstrated how the improved statistical model adds critical insight to the performance of the model.","tags":null,"title":"Enhancing Model Predictability for a Scramjet Using Probabilistic Learning on Manifolds","type":"publication"},{"authors":["K. Chowdhary","C. Safta","H.N. Najm"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"9920e48be99dd1349d9b3f46016ed779","permalink":"https://cosminsafta.github.io/publication/chowdhari-2018/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/chowdhari-2018/","section":"publication","summary":"In this work, we provide a method for enhancing stochastic Galerkin moment calculations to the linear elliptic equation with random diffusivity using an ensemble of Monte Carlo solutions. This hybrid approach combines the accuracy of low-order stochastic Galerkin and the computational efficiency of Monte Carlo methods to provide statistical moment estimates which are significantly more accurate than performing each method individually. The hybrid approach involves computing a low-order stochastic Galerkin solution, after which Monte Carlo techniques are used to estimate the residual. We show that the combined stochastic Galerkin solution and residual is superior in both time and accuracy for a one-dimensional test problem and a more computational intensive two-dimensional linear elliptic problem for both the mean and variance quantities.","tags":null,"title":"Enhancing statistical moment calculations for stochastic Galerkin solutions with Monte Carlo techniques","type":"publication"},{"authors":["P. Tsilifis","X. Huan","C. Safta","K. Sargsyan","G. Lacaze","J.C. Oefelein","H.N. Najm","R.G. Ghanem"],"categories":null,"content":"","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"8443cc9dbbd43bab322727f36fd64263","permalink":"https://cosminsafta.github.io/publication/tsilifis-2019/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/tsilifis-2019/","section":"publication","summary":"Basis adaptation in Homogeneous Chaos spaces rely on a suitable rotation of the underlying Gaussian germ. Several rotations have been proposed in the literature resulting in adaptations with different convergence properties. In this paper we present a new adaptation mechanism that builds on compressive sensing algorithms, resulting in a reduced polynomial chaos approximation with optimal sparsity. The developed adaptation algorithm consists of a two-step optimization procedure that computes the optimal coefficients and the input projection matrix of a low dimensional chaos expansion with respect to an optimally rotated basis. We demonstrate the attractive features of our algorithm through several numerical examples including the application on Large-Eddy Simulation (LES) calculations of turbulent combustion in a HIFiRE scramjet engine.","tags":null,"title":"Compressive Sensing Adaptation for Polynomial Chaos Expansions","type":"publication"},{"authors":["M. Vohra","A. Alexanderian","C. Safta","S. Mahadevan"],"categories":null,"content":"","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"7f51bfb8b53440a12b6748a0837d4fe5","permalink":"https://cosminsafta.github.io/publication/vohra-2018/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/vohra-2018/","section":"publication","summary":"Surrogate modeling has become a critical component of scientific computing in situations involving expensive model evaluations. However, training a surrogate model can be remarkably challenging and even computationally prohibitive in the case of intensive simulations and large-dimensional systems. We develop a systematic approach for surrogate model construction in reduced input parameter spaces. A sparse set of model evaluations in the original input space is used to approximate derivative based global sensitivity measures (DGSMs) for individual uncertain inputs of the model. An iterative screening procedure is developed that exploits DGSM estimates in order to identify the unimportant inputs. The screening procedure forms an integral part of an overall framework for adaptive construction of a surrogate in the reduced space. The framework is tested for computational efficiency through an initial implementation in simple test cases such as the classic Borehole function, and a semilinear elliptic PDE with a random source function. The framework is then deployed for a realistic application from chemical kinetics, where we study the ignition delay in an H2/O2 reaction mechanism with 19 and 33 uncertain rate-controlling parameters. It is observed that significant computational gains can be attained by constructing accurate low-dimensional surrogates using the proposed framework.","tags":null,"title":"Sensitivity-Driven Adaptive Construction of Reduced-space Surrogates","type":"publication"},{"authors":["M. Lucchesi","H.H. Alzahrani","C. Safta","O.M. Knio"],"categories":null,"content":"","date":1538524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538524800,"objectID":"06cc9e9dfaa4b0839d22699a863b2f5d","permalink":"https://cosminsafta.github.io/publication/lucchesi-2019/","publishdate":"2019-03-13T00:00:00Z","relpermalink":"/publication/lucchesi-2019/","section":"publication","summary":"We present a new strategy to couple, in a non-split fashion, stiff integration schemes with explicit, extended-stability predictor-corrector methods. The approach is illustrated through the construction of a mixed scheme incorporating a stabilised second-order, Runge-Kutta-Chebyshev method and the CVODE stiff solver. The scheme is first applied to an idealised stiff reaction-diffusion problem that admits an analytical solution. Analysis of the computations reveals that as expected the scheme exhibits a second-order in time convergence, and that, compared to an operator-split construction, time integration errors are substantially reduced. The non-split scheme is then applied to model the transient evolution of a freely-propagating, 1D methane-air flame. A low-mach-number, detailed kinetics, combustion model, discretised in space using fourth-order differences, is used for this purpose. To assess the performance of the scheme, self-convergence tests are conducted, and the results are contrasted with computations performed using a Strang-split construction. Whereas both the split and non-split approaches exhibit second-order in time behaviour, it is seen that for the same value of the time step, the non-split approach exhibits significantly smaller time integration errors. On the other hand, the results also indicate that the application of the present non-split construction becomes attractive when large integration steps are used, due to numerical overhead.","tags":null,"title":"A hybrid, non-split, stiff/RKC, solver for advection–diffusion–reaction equations and its application to low-Mach number combustion","type":"publication"},{"authors":["J. Cheng","R.L.-Y. Chen","H.N. Najm","A. Pinar","C. Safta","J.-P. Watson"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"2065b75d41d2624cad45f66324a73ec1","permalink":"https://cosminsafta.github.io/publication/cheng-2018b/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/cheng-2018b/","section":"publication","summary":"Increasing penetration levels of renewables have transformed how power systems are operated. High levels of uncertainty in production make it increasingly difficulty to guarantee operational feasibility; instead, constraints may only be satisfied with high probability. We present a chance-constrained economic dispatch model that efficiently integrates energy storage and high renewable penetration to satisfy renewable portfolio requirements. Specifically, we require that wind energy contribute at least a prespecified proportion of the total demand and that the scheduled wind energy is deliverable with high probability. We develop an approximate partial sample average approximation (PSAA) framework to enable efficient solution of large-scale chance-constrained economic dispatch problems. Computational experiments on the IEEE-24 bus system show that the proposed PSAA approach is more accurate, closer to the prescribed satisfaction tolerance, and approximately 100 times faster than standard sample average approximation. Finally, the improved efficiency of our PSAA approach enables solution of a larger WECC-240 test system in minutes.","tags":null,"title":"Chance-Constrained Economic Dispatch with Renewable Energy and Storage","type":"publication"},{"authors":["F. Rizzi","K. Morris","K. Sargsyan","P. Mycek","C. Safta","O. Le Matre","O.M. Knio","B.J. Debuschere"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"637b22a94ac1d526ceeb7f4790d78a83","permalink":"https://cosminsafta.github.io/publication/rizzi-2017a/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/publication/rizzi-2017a/","section":"publication","summary":"Task-based PDE preconditioner resilient to silent data corruptions (SDCs).Demonstrate excellent scalability and suitability to run at large scale.Demonstrate potential energy savings via dynamics voltage/frequency scaling.Demonstrate resilience to SDCs stemming from single and double bit-flips. We discuss algorithm-based resilience to silent data corruptions (SDCs) in a task-based domain-decomposition preconditioner for partial differential equations (PDEs). The algorithm exploits a reformulation of the PDE as a sampling problem, followed by a solution update through data manipulation that is resilient to SDCs. The implementation is based on a server-client model where all state information is held by the servers, while clients are designed solely as computational units. Scalability tests run up to 51K cores show a parallel efficiency greater than 90%. We use a 2D elliptic PDE and a fault model based on random single and double bit-flip to demonstrate the resilience of the application to synthetically injected SDC. We discuss two fault scenarios: one based on the corruption of all data of a target task, and the other involving the corruption of a single data point. We show that for our application, given the test problem considered, a four-fold increase in the number of faults only yields a 2% change in the overhead to overcome their presence, from 7% to 9%. We then discuss potential savings in energy consumption via dynamic voltage/frequency scaling, and its interplay with fault-rates, and application overhead.","tags":null,"title":"Exploring the Interplay of Resilience and Energy Consumption for a Task-Based Partial Differential Equations Preconditioner","type":"publication"},{"authors":["X. Huan","C. Safta","K. Sargsyan","Z.P. Vane","G. Lacaze","J.C. Oefelein","H.N. Najm,"],"categories":null,"content":"","date":1522108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522108800,"objectID":"e05e712811809445c8ba79728ae9efda","permalink":"https://cosminsafta.github.io/publication/huan-2018/","publishdate":"2018-03-27T00:00:00Z","relpermalink":"/publication/huan-2018/","section":"publication","summary":"Compressive sensing is a powerful technique for recovering sparse solutions of underdetermined linear systems, which is often encountered in uncertainty quantification analysis of expensive and high-dimensional physical models. We perform numerical investigations employing several compressive sensing solvers that target the unconstrained LASSO formulation, with a focus on linear systems that arise in the construction of polynomial chaos expansions. With core solvers l1_ls, SpaRSA, CGIST, FPC_AS, and ADMM, we develop techniques to mitigate overfitting through an automated selection of regularization constant based on cross-validation, and a heuristic strategy to guide the stop-sampling decision. Practical recommendations on parameter settings for these techniques are provided and discussed. The overall method is applied to a series of numerical examples of increasing complexity, including large eddy simulations of supersonic turbulent jet-in-crossflow involving a 24-dimensional input. Through empirical phase-transition diagrams and convergence plots, we illustrate sparse recovery performance under structures induced by polynomial chaos, accuracy, and computational trade-offs between polynomial bases of different degrees, and practicability of conducting compressive sensing for a realistic, high-dimensional physical application. Across test cases studied in this paper, we find ADMM to have demonstrated empirical advantages through consistent lower errors and faster computational times.","tags":null,"title":"Compressive Sensing with Cross-Validation and Stop-Sampling for Sparse Polynomial Chaos Expansions","type":"publication"},{"authors":["J. Cheng","R.L.-Y. Chen","H.N. Najm","A. Pinar","C. Safta","J.-P. Watson"],"categories":null,"content":"","date":1521590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521590400,"objectID":"4a4a75c9639bf1f89332b419dd11a125","permalink":"https://cosminsafta.github.io/publication/cheng-2018/","publishdate":"2018-03-21T00:00:00Z","relpermalink":"/publication/cheng-2018/","section":"publication","summary":"Distributionally robust optimization (DRO) is widely used because it offers a way to overcome the conservativeness of robust optimization without requiring the specificity of stochastic programming. On the computational side, many practical DRO instances can be equivalently (or approximately) formulated as semidefinite programming (SDP) problems via conic duality of the moment problem. However, despite being theoretically solvable in polynomial time, SDP problems in practice are computationally challenging and quickly become intractable with increasing problem sizes. We propose a new approximation method to solve DRO problems with moment-based ambiguity sets. Our approximation method relies on principal component analysis (PCA) for optimal lower dimensional representation of variability in random samples. We show that the PCA approximation yields a relaxation of the original problem and derive theoretical bounds on the gap between the original problem and its PCA approximation. Furthermore, an extensive numerical study shows the strength of the proposed approximation method in terms of solution quality and runtime. As examples, for distributionally robust conditional value-at-risk and risk-averse production-transportation problems the proposed PCA approximation using only 50% of the principal components yields near-optimal solutions (within 1%) with a one to two order of magnitude reduction in computation time.","tags":null,"title":"Distributionally Robust Optimization with Principal Component Analysis","type":"publication"},{"authors":["X. Huan","C. Safta","K. Sargsyan","G. Geraci","M.S. Eldred","Z.P. Vane","G. Lacaze","J.C. Oefelein","H.N. Najm,"],"categories":null,"content":"","date":1518393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518393600,"objectID":"9683fcec81d7cd90dc5a9b2849b4e277","permalink":"https://cosminsafta.github.io/publication/huan-2018b/","publishdate":"2018-02-12T00:00:00Z","relpermalink":"/publication/huan-2018b/","section":"publication","summary":"The development of scramjet engines is an important research area for advancing hypersonic and orbital flights. Progress toward optimal engine designs requires accurate flow simulations together with uncertainty quantification. However, performing uncertainty quantification for scramjet simulations is challenging due to the large number of uncertain parameters involved and the high computational cost of flow simulations. These difficulties are addressed in this paper by developing practical uncertainty quantification algorithms and computational methods, and deploying them in the current study to large-eddy simulations of a jet in crossflow inside a simplified HIFiRE Direct Connect Rig scramjet combustor. First, global sensitivity analysis is conducted to identify influential uncertain input parameters, which can help reduce the system’s stochastic dimension. Second, because models of different fidelity are used in the overall uncertainty quantification assessment, a framework for quantifying and propagating the uncertainty due to model error is presented. These methods are demonstrated on a nonreacting jet-in-crossflow test problem in a simplified scramjet geometry, with parameter space up to 24 dimensions, using static and dynamic treatments of the turbulence subgrid model, and with two-dimensional and three-dimensional geometries.","tags":null,"title":"Global Sensitivity Analysis and Estimation of Model Error, Toward Uncertainty Quantification in Scramjet Computations","type":"publication"},{"authors":["P. Mycek","A. Contreras","O. Le Maitre","K. Sargsyan","F. Rizzi","K. Morris","C. Safta","B.J. Debuschere","O.M. Knio"],"categories":null,"content":"","date":1487116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487116800,"objectID":"a38369947264cd60b2ccb720e57318fe","permalink":"https://cosminsafta.github.io/publication/mycek-2017a/","publishdate":"2018-02-24T00:00:00Z","relpermalink":"/publication/mycek-2017a/","section":"publication","summary":"A resilient method is developed for the solution of uncertain elliptic PDEs on extreme scale platforms. The method is based on a hybrid domain decomposition, polynomial chaos (PC) framework that is designed to address soft faults. Specifically, parallel and independent solves of multiple deterministic local problems are used to define PC representations of local Dirichlet boundary-to-boundary maps that are used to reconstruct the global solution. A LAD-lasso type regression is developed for this purpose. The performance of the resulting algorithm is tested on an elliptic equation with an uncertain diffusivity field. Different test cases are considered in order to analyze the impacts of correlation structure of the uncertain diffusivity field, the stochastic resolution, as well as the probability of soft faults. In particular, the computations demonstrate that, provided sufficiently many samples are generated, the method effectively overcomes the occurrence of soft faults.","tags":null,"title":"A resilient domain decomposition polynomial chaos solver for uncertain elliptic PDEs","type":"publication"},{"authors":["D. Lu","D. Ricciuto","A. Walker","C. Safta","W. Munger"],"categories":null,"content":"","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486684800,"objectID":"b17337564e04dfc8e8456059d1f73e13","permalink":"https://cosminsafta.github.io/publication/lu-2017/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/publication/lu-2017/","section":"publication","summary":"Calibration of terrestrial ecosystem models is important but challenging. Bayesian inference implemented by Markov chain Monte Carlo (MCMC) sampling provides a comprehensive framework to estimate model parameters and associated uncertainties using their posterior distributions. The effectiveness and efficiency of the method strongly depend on the MCMC algorithm used. In this work, a differential evolution adaptive Metropolis (DREAM) algorithm is used to estimate posterior distributions of 21 parameters for the data assimilation linked ecosystem carbon (DALEC) model using 14 years of daily net ecosystem exchange data collected at the Harvard Forest Environmental Measurement Site eddy-flux tower. The calibration of DREAM results in a better model fit and predictive performance compared to the popular adaptive Metropolis (AM) scheme. Moreover, DREAM indicates that two parameters controlling autumn phenology have multiple modes in their posterior distributions while AM only identifies one mode. The application suggests that DREAM is very suitable to calibrate complex terrestrial ecosystem models, where the uncertain parameter size is usually large and existence of local optima is always a concern. In addition, this effort justifies the assumptions of the error model used in Bayesian calibration according to the residual analysis. The result indicates that a heteroscedastic, correlated, Gaussian error model is appropriate for the problem, and the consequent constructed likelihood function can alleviate the underestimation of parameter uncertainty that is usually caused by using uncorrelated error models.","tags":null,"title":"Bayesian calibration of terrestrial ecosystem models: a study of advanced Markov chain Monte Carlo methods","type":"publication"},{"authors":["F. Rizzi","K. Morris","K. Sargsyan","P. Mycek","C. Safta","O. Le Matre","O. Knio","B.J. Debuschere"],"categories":null,"content":"","date":1485648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485648000,"objectID":"926b1215bfed36ab31905e17f85ef1ad","permalink":"https://cosminsafta.github.io/publication/rizzi-2017b/","publishdate":"2017-01-29T00:00:00Z","relpermalink":"/publication/rizzi-2017b/","section":"publication","summary":"We present a domain-decomposition-based preconditioner for the solution of partial differential equations (PDEs) that is resilient to both soft and hard faults. The algorithm reformulates the PDE as a sampling problem, followed by a solution update through data manipulation that is resilient to both soft and hard faults. This reformulation allows us to recast the problem as a set of independent tasks, and exploit data locality to reduce global communication. We discuss two different parallel implementations: (a) a single program multiple data (SPMD) version based on a one-to-one mapping between subdomain and MPI processes responsible for both state and computation; and (b) an asynchronous server–client implementation where all state information is held by the servers and clients are designed solely as computational units. We present a scalability comparison of both implementations under nominal conditions, showing efficiency within ~80% for up to 12,000 cores. We present a resilience analysis under different fault scenarios based on the server–client implementation. This framework provides resiliency to hard faults such that if a client crashes, it stops asking for work, and the servers simply distribute the work among all of the other clients alive. Erroneous subdomain solves (e.g. due to soft faults) appear as corrupted data, which is either rejected if that causes a task to fail, or is seamlessly filtered out during the regression stage through a suitable noise model. Three different types of faults are modeled: hard faults modeling nodes (or clients) crashing; soft faults occurring during the communication of the tasks between server and clients; and soft faults occurring during task execution. We demonstrate the resiliency of the approach for a 2D elliptic PDE, and explore the effect of the faults at various failure rates.","tags":null,"title":"Partial differential equations preconditioner resilient to soft and hard faults","type":"publication"},{"authors":["P. Mycek","F. Rizzi","O. Le Maitre","K. Sargsyan","K. Morris","C. Safta","B.J. Debuschere","O.M. Knio"],"categories":null,"content":"","date":1476835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476835200,"objectID":"96659a4766e1950a2d14a7431302d9fb","permalink":"https://cosminsafta.github.io/publication/mycek-2017b/","publishdate":"2017-01-05T00:00:00Z","relpermalink":"/publication/mycek-2017b/","section":"publication","summary":"A priori bounds are derived for the discrete solution of second-order elliptic partial differential equations (PDEs). The bounds have two contributions. First, the influence of boundary conditions is taken into account through a discrete maximum principle. Second, the contribution of the source field is evaluated in a fashion similar to that used in the treatment of the continuous a priori operators. Closed form expressions are, in particular, obtained for the case of a conservative, second-order finite difference approximation of the diffusion equation with variable scalar diffusivity. The bounds are then incorporated into a resilient domain decomposition framework, in order to verify the admissibility of local PDE solutions. The computations demonstrate that the bounds are able to detect most system faults, and thus considerably enhance the resilience and the overall performance of the solver.","tags":null,"title":"Discrete A Priori Bounds for the Detection of Corrupted PDE Solutions in Exascale Computations","type":"publication"},{"authors":["R. Malpica Galassi","M. Valorani","H.N. Najm","C. Safta","M. Khalil","P.P. Ciottoli"],"categories":null,"content":"","date":1476748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476748800,"objectID":"d26343d587d2dd95864ea7b638ffe6a2","permalink":"https://cosminsafta.github.io/publication/malpicagalassi-2017/","publishdate":"2017-03-06T00:00:00Z","relpermalink":"/publication/malpicagalassi-2017/","section":"publication","summary":"A general strategy for analysis and reduction of uncertain chemical kinetic models is presented, and its utility is illustrated in the context of ignition of hydrocarbon fuel–air mixtures. The strategy is based on a deterministic analysis and reduction method which employs computational singular perturbation analysis to generate simplified kinetic mechanisms, starting from a detailed reference mechanism. We model uncertain quantities in the reference mechanism, namely the Arrhenius rate parameters, as random variables with prescribed uncertainty factors. We propagate this uncertainty to obtain the probability of inclusion of each reaction in the simplified mechanism. We propose probabilistic error measures to compare predictions from the uncertain reference and simplified models, based on the comparison of the uncertain dynamics of the state variables, where the mixture entropy is chosen as progress variable. We employ the construction for the simplification of an uncertain mechanism in an n-butane–air mixture homogeneous ignition case, where a 176-species, 1111-reactions detailed kinetic model for the oxidation of n-butane is used with uncertainty factors assigned to each Arrhenius rate pre-exponential coefficient. This illustration is employed to highlight the utility of the construction, and the performance of a family of simplified models produced depending on chosen thresholds on importance and marginal probabilities of the reactions.","tags":null,"title":"Chemical model reduction under uncertainty","type":"publication"},{"authors":["C. Safta","R.L.-Y. Chen","H.N. Najm","A. Pinar","J.-P. Watson"],"categories":null,"content":"","date":1475712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475712000,"objectID":"406f6f354fc1e3bd23cdd8980b104a8c","permalink":"https://cosminsafta.github.io/publication/safta-2016a/","publishdate":"2016-10-06T00:00:00Z","relpermalink":"/publication/safta-2016a/","section":"publication","summary":"Stochastic economic dispatch models address uncertainties in forecasts of renewable generation output by considering a finite number of realizations drawn from a stochastic process model, typically via Monte Carlo sampling. Accurate evaluations of expectations or higher order moments for quantities of interest, e.g., generating cost, can require a prohibitively large number of samples. We propose an alternative to Monte Carlo sampling based on polynomial chaos expansions. These representations enable efficient and accurate propagation of uncertainties in model parameters, using sparse quadrature methods. We also use Karhunen-Loève expansions for efficient representation of uncertain renewable energy generation that follows geographical and temporal correlations derived from historical data at each wind farm. Considering expected production cost, we demonstrate that the proposed approach can yield several orders of magnitude reduction in computational cost for solving stochastic economic dispatch relative to Monte Carlo sampling, for a given target error threshold.","tags":null,"title":"Efficient Uncertainty Quantification in Stochastic Economic Dispatch","type":"publication"},{"authors":["M. Khalil","K. Chowdhary","C. Safta","K. Sargsyan","H.N. Najm"],"categories":null,"content":"","date":1471478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471478400,"objectID":"6db28d6dc19fc9226a7486393056fee2","permalink":"https://cosminsafta.github.io/publication/khalil-2016/","publishdate":"2016-10-15T00:00:00Z","relpermalink":"/publication/khalil-2016/","section":"publication","summary":"We present the results of an application of Bayesian inference and maximum entropy methods for the estimation of the joint probability density for the Arrhenius rate parameters of the rate coefficient of the H2/O2-mechanism chain branching reaction H+O2→OH+O. Available published data is summary statistics in terms of nominal values and error bars of the rate coefficient of this reaction at a number of temperature values obtained from shock-tube experiments. Our approach relies on generating data, in this case OH concentration profiles, consistent with the given summary statistics, using Approximate Bayesian Computation methods and a Markov chain Monte Carlo procedure. The approach permits the forward propagation of parametric uncertainty through the computational model in a manner that is consistent with the published statistics. A consensus joint posterior on the parameters is obtained by pooling the posterior parameter densities given each consistent data set. To expedite this process, we construct efficient surrogates for the OH concentration using a combination of Padé and polynomial approximants. These surrogate models adequately represent forward model observables and their dependence on input parameters and are computationally efficient to allow their use in the Bayesian inference procedure. We also utilize Gauss–Hermite quadrature with Gaussian proposal probability density functions for moment computation resulting in orders of magnitude speedup in data likelihood evaluation. Despite the strong non-linearity in the model, the consistent data sets all result in nearly Gaussian conditional parameter probability density functions. The technique also accounts for nuisance parameters in the form of Arrhenius parameters of other rate coefficients with prescribed uncertainty. The resulting pooled parameter probability density function is propagated through stoichiometric hydrogen–air auto-ignition computations to illustrate the need to account for correlation among the Arrhenius rate parameters of one reaction and across rate parameters of different reactions.","tags":null,"title":"Inference of reaction rate parameters based on summary statistics from experiments","type":"publication"},{"authors":["C. Safta","M. Blaylock","J. Templeton","S. Domino","K. Sargsyan","H.N. Najm"],"categories":null,"content":"","date":1466208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466208000,"objectID":"87801576d17f68d47a50555670260175","permalink":"https://cosminsafta.github.io/publication/safta-2016b/","publishdate":"2016-06-23T00:00:00Z","relpermalink":"/publication/safta-2016b/","section":"publication","summary":"In this paper, we present a Bayesian framework for estimating joint densities for large eddy simulation (LES) sub-grid scale model parameters based on canonical forced isotropic turbulence direct numerical simulation (DNS) data. The framework accounts for noise in the independent variables, and we present alternative formulations for accounting for discrepancies between model and data. To generate probability densities for flow characteristics, posterior densities for sub-grid scale model parameters are propagated forward through LES of channel flow and compared with DNS data. Synthesis of the calibration and prediction results demonstrates that model parameters have an explicit filter width dependence and are highly correlated. Discrepancies between DNS and calibrated LES results point to additional model form inadequacies that need to be accounted for.","tags":null,"title":"Uncertainty quantification in LES of channel flow","type":"publication"},{"authors":["K. Sargsyan","F. Rizzi","P. Mycek","C. Safta","K. Morris","H.N. Najm","O. Le Maitre","O. Knio","B. Debusschere"],"categories":null,"content":"","date":1437004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437004800,"objectID":"b773f36853b3172c51425f8fdc6899f0","permalink":"https://cosminsafta.github.io/publication/sargsyan-2015/","publishdate":"2015-09-10T00:00:00Z","relpermalink":"/publication/sargsyan-2015/","section":"publication","summary":"The move towards extreme-scale computing platforms challenges scientific simulations in many ways. Given the recent tendencies in computer architecture development, one needs to reformulate legacy codes in order to cope with large amounts of communication, system faults, and requirements of low-memory usage per core. In this work, we develop a novel framework for solving PDEs via domain decomposition that reformulates the solution as a state of knowledge with a probabilistic interpretation. Such reformulation allows resiliency with respect to potential faults without having to apply fault detection, avoids unnecessary communication, and is generally well-suited for rigorous uncertainty quantification studies that target improvements of predictive fidelity of scientific models. We demonstrate our algorithm for one-dimensional PDE examples where artificial faults have been implemented as bit flips in the binary representation of subdomain solutions.","tags":null,"title":"Fault Resilient Domain Decomposition Preconditioner for PDEs","type":"publication"},{"authors":["C. Safta","D. Ricciuto","K. Sargsyan","D. Debusschere","H.N. Najm","M. Williams","P.E. Thornton"],"categories":null,"content":"","date":1433808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433808000,"objectID":"ba8ab6bd562869c80aaa217fdf06a65c","permalink":"https://cosminsafta.github.io/publication/safta-2015a/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/publication/safta-2015a/","section":"publication","summary":"In this paper we propose a probabilistic framework for an uncertainty quantification (UQ) study of a carbon cycle model and focus on the comparison between steady-state and transient simulation setups. A global sensitivity analysis (GSA) study indicates the parameters and parameter couplings that are important at different times of the year for quantities of interest (QoIs) obtained with the data assimilation linked ecosystem carbon (DALEC) model. We then employ a Bayesian approach and a statistical model error term to calibrate the parameters of DALEC using net ecosystem exchange (NEE) observations at the Harvard Forest site. The calibration results are employed in the second part of the paper to assess the predictive skill of the model via posterior predictive checks.","tags":null,"title":"Global sensitivity analysis, probabilistic calibration, and predictive assessment for the data assimilation linked ecosystem carbon model","type":"publication"},{"authors":["C. Safta","K. Sargsyan","D. Debusschere","H.N. Najm"],"categories":null,"content":"","date":1413072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413072000,"objectID":"a01635bb805c423d7f95b3e571d8e563","permalink":"https://cosminsafta.github.io/publication/safta-2015b/","publishdate":"2014-10-23T00:00:00Z","relpermalink":"/publication/safta-2015b/","section":"publication","summary":"Direct solutions of the Chemical Master Equation (CME) governing Stochastic Reaction Networks (SRNs) are generally prohibitively expensive due to excessive numbers of possible discrete states in such systems. To enhance computational efficiency we develop a hybrid approach where the evolution of states with low molecule counts is treated with the discrete CME model while that of states with large molecule counts is modeled by the continuum Fokker–Planck equation. The Fokker–Planck equation is discretized using a 2nd order finite volume approach with appropriate treatment of flux components. The numerical construction at the interface between the discrete and continuum regions implements the transfer of probability reaction by reaction according to the stoichiometry of the system. The performance of this novel hybrid approach is explored for a two-species circadian model with computational efficiency gains of about one order of magnitude.","tags":null,"title":"Hybrid discrete/continuum algorithms for stochastic reaction networks","type":"publication"},{"authors":["C. Safta","K. Sargsyan","H.N. Najm","K. Chowdhary","B. Debusschere","L.P. Swiler","M.S. Eldred"],"categories":null,"content":"","date":1407801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1407801600,"objectID":"fd2bb1814d3f0986cd54e48f601a23bf","permalink":"https://cosminsafta.github.io/publication/safta-2014a/","publishdate":"2015-02-06T00:00:00Z","relpermalink":"/publication/safta-2014a/","section":"publication","summary":"In this paper, a series of algorithms are proposed to address the problems in the NASA Langley Research Center Multidisciplinary Uncertainty Quantification Challenge. A Bayesian approach is employed to characterize and calibrate the epistemic parameters based on the available data, whereas a variance-based global sensitivity analysis is used to rank the epistemic and aleatory model parameters. A nested sampling of the aleatory–epistemic space is proposed to propagate uncertainties from model parameters to output quantities of interest.","tags":null,"title":"Probabilistic Methods for Sensitivity Analysis and Calibration in the NASA Challenge Problem","type":"publication"}]